"""
Gemini AI Analyzer Service
ä½¿ç”¨ Google Gemini è¿›è¡Œäº‹ä»¶åˆ†æå’Œæ¦‚ç‡æ ¡å‡†
"""

import os
import re
import json
import asyncio
import logging
from datetime import datetime
from typing import Optional, Dict, Any

import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

logger = logging.getLogger(__name__)


def _fix_json_string(text: str) -> str:
    """
    å°è¯•ä¿®å¤å¸¸è§çš„ JSON æ ¼å¼é—®é¢˜
    """
    # 1. ç§»é™¤ markdown ä»£ç å—æ ‡è®°
    text = re.sub(r'^```json\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'^```\s*$', '', text, flags=re.MULTILINE)
    text = text.strip()
    
    # 2. ç§»é™¤å°¾éƒ¨é€—å· (trailing commas)
    text = re.sub(r',(\s*[}\]])', r'\1', text)
    
    # 3. ä¿®å¤å•å¼•å·ä¸ºåŒå¼•å· (ç®€å•æƒ…å†µ)
    # æ³¨æ„ï¼šè¿™æ˜¯ç²—æš´å¤„ç†ï¼Œå¯èƒ½åœ¨æŸäº›è¾¹ç¼˜æƒ…å†µå¤±æ•ˆ
    
    return text


class GeminiAnalyzer:
    def __init__(self):
        self.api_key = os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            logger.warning("âš ï¸ GEMINI_API_KEY not set. AI analysis will fail.")
        else:
            genai.configure(api_key=self.api_key)

    def _get_model(self):
        """é…ç½® Gemini æ¨¡å‹"""
        generation_config = {
            "temperature": 0.7,
            "response_mime_type": "application/json",  # å¼ºåˆ¶è¾“å‡º JSON
        }

        return genai.GenerativeModel(
            model_name="gemini-2.0-flash",  # ç¨³å®šå¯ç”¨çš„æ¨¡å‹
            generation_config=generation_config,
            safety_settings={
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }
        )

    def _get_market_probability(self, market: Dict[str, Any]) -> float:
        """æå–å¸‚åœºæ¦‚ç‡ï¼ˆç»Ÿä¸€é€»è¾‘ï¼‰"""
        if "calculated_odds" in market:
            return float(market["calculated_odds"])
        
        outcome_prices = market.get("outcomePrices", [])
        if outcome_prices:
            try:
                if isinstance(outcome_prices, str):
                    outcome_prices = json.loads(outcome_prices)
                return float(outcome_prices[0])
            except:
                pass
        
        return float(market.get("probability", 0.0))

    def _construct_prompt(self, event_data: Dict[str, Any]) -> str:
        """
        æ„å»º Prompt (V6ï¼šå®Œæ•´é¢„å¤„ç† + 5% é—¨æ§› + å…œåº•/ä¸Šé™)
        """
        current_time = datetime.utcnow().strftime("%Y-%m-%d %H:%M UTC")
        
        # === 1. å¸‚åœºé¢„å¤„ç†ï¼ˆèåˆ preprocess_event é€»è¾‘ï¼‰ ===
        MIN_ODDS_THRESHOLD = 0.05  # 5% é—¨æ§›
        MIN_MARKETS = 2            # æœ€å°‘ä¿ç•™æ•°é‡
        MAX_MARKETS = 5            # æœ€å¤šä¿ç•™æ•°é‡
        
        raw_markets = event_data.get("markets", [])
        
        # Step 1: è¿‡æ»¤ä¸å¯äº¤æ˜“çš„å¸‚åœºï¼ˆarchived/inactive/closedï¼‰
        eligible_markets = []
        for m in raw_markets:
            if m.get("archived") is True:
                continue
            if m.get("active") is not True:
                continue
            if m.get("closed") is True:
                continue
            eligible_markets.append(m)
        
        # Step 2: è®¡ç®—èµ”ç‡å¹¶æ’åºï¼ˆé™åºï¼‰
        markets_with_odds = []
        for m in eligible_markets:
            odds = self._get_market_probability(m)
            markets_with_odds.append({
                "market": m,
                "odds": odds,
                "market_id": m.get("id", m.get("polymarket_id", "")),
                "question": m.get("question", ""),
            })
        markets_with_odds.sort(key=lambda x: x["odds"], reverse=True)
        
        # Step 3: ä¸»è¿‡æ»¤ - 5% é—¨æ§›
        filtered_markets = [m for m in markets_with_odds if m["odds"] >= MIN_ODDS_THRESHOLD]
        
        # Step 4: å…œåº• & ä¸Šé™
        if len(filtered_markets) < MIN_MARKETS:
            # ä¸è¶³ 2 ä¸ªï¼Œå–å‰ 2ï¼ˆå³ä½¿ < 5%ï¼‰
            selected_markets = markets_with_odds[:MIN_MARKETS]
            logger.info(f"ğŸ“Š ä¸è¶³ {MIN_MARKETS} ä¸ªå¸‚åœºæ»¡è¶³ 5% é—¨æ§›ï¼Œå…œåº•å–å‰ {MIN_MARKETS}")
        elif len(filtered_markets) > MAX_MARKETS:
            # è¶…è¿‡ 5 ä¸ªï¼Œåªå–å‰ 5
            selected_markets = filtered_markets[:MAX_MARKETS]
            logger.info(f"ğŸ“Š è¶…è¿‡ {MAX_MARKETS} ä¸ªå¸‚åœºæ»¡è¶³é—¨æ§›ï¼Œæˆªå–å‰ {MAX_MARKETS}")
        else:
            selected_markets = filtered_markets
            logger.info(f"ğŸ“Š {len(selected_markets)} ä¸ªå¸‚åœºè¿›å…¥ AI åˆ†ææ± ")
        
        # Step 5: æ„å»º markets_text
        markets_text = ""
        for item in selected_markets:
            odds = item["odds"]
            markets_text += f"""
            - Market ID: {item["market_id"]}
            - Question: {item["question"]}
            - Current Probability: {odds:.2f} ({odds*100:.1f}%)
            """

        # 2. V4 æ ¸å¿ƒ Promptï¼šå®¡è®¡å‘˜ + é”šå®šæ•ˆåº” + ä¸¥æ ¼çº¦æŸ
        prompt = f"""
        Role: You are a Red-Team Forecaster. Your goal is to analyze a Polymarket Event and its associated markets to provide a "Skeptical Calibration" of the odds.

        Input Format: You will receive an Event Title, Event Description, and a list of Markets (each with its own Question, Description, and Current Odds).

        ---
        Analytical Process (Red-Team Logic)
        For the overall Event and each specific Market, use Google Search to investigate:
        1. The Event Strategy (Global): Identify the overarching macro-tension (e.g., Regulatory environment, legal timelines, or broad political trends).
        2. Structural Reality (The Anchor): Find hard data (laws, SEC filings, official OPM procedures) that contradicts current market pricing.
        3. The Blindspot (Calibration): Why is the crowd wrong? Look for "Headline Confusion" where traders bet on news rather than the legal resolution criteria.

        IMPORTANT: Use Google Search to find current information, official documents, and hard data to support your analysis.
        IMPORTANT: Current datetime (minute-accurate): {current_time}

        Input Event:
        Title: {event_data.get("title", "")}
        Description: {event_data.get("description", "")}
                
        Markets:
        {markets_text}

        OUTPUT :
        Please provide the response in the following structure:
        1. Executive AI Event Summary
        [Write ONE precise sentence (MAX 18 words) capturing the macro-anchor governing the entire event.]
        ---
        2. Individual Market Calibrations
        For each market provided in the input, generate a separate analysis block:
        Market: [Market Question]
        - AI Calibrated Odds: [Your %] 
        - The Structural Anchor: [One sentence explaining the primary hard-data constraint for this specific market.] 

        OUTPUT FORMAT (Strict JSON):
        {{
            "executive_summary": "string",
            "markets": {{
                "MARKET_ID_1": {{
                    "ai_calibrated_odds": 0.65, 
                }}, "MARKET_ID_2": {{
                    "ai_calibrated_odds": 0.35,
                }}, "MARKET_ID_3": {{
                    "ai_calibrated_odds": 0.0,
                }},
            }}
        }}
        """
        return prompt

    def analyze_with_gemini(self, event_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        åŒæ­¥ç‰ˆæœ¬ï¼šåˆ†æå•ä¸ªäº‹ä»¶ï¼ˆå¸¦è¾“å…¥è¾“å‡ºå®¡è®¡æ—¥å¿—ï¼‰
        """
        if not self.api_key:
            logger.error("âŒ GEMINI_API_KEY not configured")
            return None

        model = self._get_model()
        prompt = self._construct_prompt(event_data)

        # --- [æ£€ç´¢ç‚¹ 1: è¾“å…¥å®¡è®¡] ---
        logger.debug(f"===== AI INPUT PROMPT (Event: {event_data.get('id')}) =====")
        logger.debug(prompt)

        try:
            response = model.generate_content(prompt)
            raw_response = response.text

            # --- [æ£€ç´¢ç‚¹ 2: è¾“å‡ºå®¡è®¡] ---
            logger.debug(f"===== AI RAW RESPONSE =====")
            logger.debug(raw_response)

            # å°è¯•è§£æ JSON
            try:
                parsed_data = json.loads(raw_response)
                return parsed_data
            except json.JSONDecodeError:
                # å°è¯•ä¿®å¤å¹¶é‡æ–°è§£æ
                fixed_text = _fix_json_string(raw_response)
                try:
                    parsed_data = json.loads(fixed_text)
                    logger.warning("âš ï¸ JSON was malformed, auto-fixed successfully")
                    return parsed_data
                except json.JSONDecodeError as e:
                    logger.error(f"è§£æ AI å›å¤å¤±è´¥: {e}, åŸå§‹æ–‡æœ¬: {raw_response}")
                    return None
        except Exception as e:
            logger.error(f"Gemini API è°ƒç”¨å¤±è´¥: {e}")
            return None

    async def analyze_event(self, event_data: Dict[str, Any], max_retries: int = 3, retry_delay: float = 2.0) -> Optional[Dict[str, Any]]:
        """
        ä¸»å…¥å£ï¼šåˆ†æå•ä¸ªäº‹ä»¶ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            event_data: åŒ…å« title, description, markets ç­‰å­—æ®µçš„äº‹ä»¶æ•°æ®
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ 3 æ¬¡
            retry_delay: é‡è¯•é—´éš”ç§’æ•°ï¼Œé»˜è®¤ 2 ç§’
            
        Returns:
            åˆ†æç»“æœå­—å…¸ï¼Œæ ¼å¼ï¼š
            {
                "executive_summary": "...",
                "markets": {
                    "market_id": {
                        "ai_calibrated_odds": 0.55,
                        "confidence_score": 8.5,
                        "analysis": {
                            "structural_anchor": "...",
                            "noise": "...",
                            "barrier": "...",
                            "blindspot": "..."
                        }
                    }
                }
            }
        """
        if not self.api_key:
            logger.error("âŒ GEMINI_API_KEY not configured")
            return None

        event_title = event_data.get("title", "Unknown")
        model = self._get_model()
        prompt = self._construct_prompt(event_data)
        
        last_error = None
        
        for attempt in range(1, max_retries + 1):
            try:
                logger.info(f"ğŸ¤– Calling Gemini for: {event_title[:30]}... (attempt {attempt}/{max_retries})")
                
                # å¼‚æ­¥è°ƒç”¨ Gemini
                response = await model.generate_content_async(prompt)
                
                # è§£æ JSON (å¸¦å®¹é”™)
                raw_text = response.text
                try:
                    result_json = json.loads(raw_text)
                except json.JSONDecodeError:
                    # å°è¯•ä¿®å¤å¹¶é‡æ–°è§£æ
                    fixed_text = _fix_json_string(raw_text)
                    try:
                        result_json = json.loads(fixed_text)
                        logger.warning("âš ï¸ JSON was malformed, auto-fixed successfully")
                    except json.JSONDecodeError as e2:
                        # JSON è§£æå¤±è´¥ï¼Œè®°å½•å¹¶é‡è¯•
                        logger.warning(f"âš ï¸ JSON parse failed (attempt {attempt}): {e2}")
                        last_error = e2
                        if attempt < max_retries:
                            await asyncio.sleep(retry_delay)
                        continue
                
                logger.info("âœ… Gemini analysis complete.")
                return result_json

            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ Gemini call failed (attempt {attempt}): {e}")
                if attempt < max_retries:
                    await asyncio.sleep(retry_delay)
                continue
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        logger.error(f"âŒ Gemini Analysis Failed after {max_retries} attempts: {last_error}")
        return None

    def transform_to_raw_analysis(
        self, 
        gemini_result: Dict[str, Any], 
        original_markets: list = None
    ) -> Dict[str, Any]:
        """
        å°† Gemini è¿”å›ç»“æœè½¬æ¢ä¸º raw_analysis å­˜å‚¨æ ¼å¼ï¼ˆå¸¦å½’ä¸€åŒ–ï¼‰
        
        Args:
            gemini_result: Gemini API è¿”å›çš„åŸå§‹ç»“æœ
            original_markets: åŸå§‹å¸‚åœºåˆ—è¡¨ï¼ˆåŒ…å«æœªè¿›å…¥ AI åˆ†ææ± çš„å¸‚åœºï¼‰
            
        Returns:
            é€‚åˆå­˜å…¥ AIPrediction.raw_analysis çš„æ ¼å¼ï¼ˆç¡®ä¿æ‰€æœ‰ Market ID éƒ½æœ‰è¿”å›ï¼‰
        """
        if not gemini_result:
            return {}
        
        ai_markets = gemini_result.get("markets", {})
        original_markets = original_markets or []
        
        # 1. æ”¶é›†æ‰€æœ‰åŸå§‹å¸‚åœºçš„æ¦‚ç‡ï¼ˆç”¨äºæœªåˆ†æå¸‚åœºçš„æå°å€¼åˆ†é…ï¼‰
        all_market_probs = {}
        for m in original_markets:
            market_id = m.get("id", m.get("polymarket_id", ""))
            prob = self._get_market_probability(m)
            all_market_probs[market_id] = prob
        
        # 2. è®¡ç®— AI è¿”å›çš„æ¦‚ç‡æ€»å’Œ
        total_ai_prob = sum(m.get("ai_calibrated_odds", 0) for m in ai_markets.values())
        
        # 3. è®¡ç®—æœªåˆ†æå¸‚åœºçš„åŸå§‹æ¦‚ç‡æ€»å’Œï¼ˆç”¨äºåˆ†é…å‰©ä½™æ¦‚ç‡ï¼‰
        analyzed_ids = set(ai_markets.keys())
        unanalyzed_prob_sum = sum(
            prob for mid, prob in all_market_probs.items() 
            if mid not in analyzed_ids
        )
        
        # æ—¥å¿—
        if total_ai_prob > 0 and abs(total_ai_prob - 1.0) > 0.01:
            logger.warning(f"âš ï¸ AI æ¦‚ç‡æ€»å’Œä¸º {total_ai_prob:.3f}ï¼Œå°†å¼ºåˆ¶å½’ä¸€åŒ–")
        if unanalyzed_prob_sum > 0:
            logger.info(f"ğŸ“Š æœªåˆ†æå¸‚åœºåŸå§‹æ¦‚ç‡æ€»å’Œ: {unanalyzed_prob_sum:.3f}")
        
        # 4. å½’ä¸€åŒ–åŸºå‡† = AI åˆ†æçš„ + æœªåˆ†æå¸‚åœºçš„åŸå§‹æ¦‚ç‡
        normalization_base = total_ai_prob + unanalyzed_prob_sum
        if normalization_base <= 0:
            normalization_base = 1.0  # é˜²æ­¢é™¤é›¶
        
        raw_analysis = {}
        
        # 5. å¤„ç† AI åˆ†æè¿‡çš„å¸‚åœº
        for market_id, market_data in ai_markets.items():
            analysis = market_data.get("analysis", {})
            calibrated_prob = market_data.get("ai_calibrated_odds", 0)
            normalized_pct = (calibrated_prob / normalization_base) * 100
            
            raw_analysis[market_id] = {
                "ai_calibrated_odds_pct": round(normalized_pct, 2),
                "ai_confidence": market_data.get("confidence_score", 0),
                "structural_anchor": analysis.get("structural_anchor"),
                "noise": analysis.get("noise"),
                "barrier": analysis.get("barrier"),
                "blindspot": analysis.get("blindspot"),
                "_analyzed": True,  # æ ‡è®°ï¼šå·²è¢« AI åˆ†æ
            }
        
        # 6. å¤„ç†æœªåˆ†æçš„å¸‚åœºï¼ˆä½äº 5% é—¨æ§›ï¼‰
        for market_id, original_prob in all_market_probs.items():
            if market_id not in analyzed_ids:
                # ä½¿ç”¨åŸå§‹æ¦‚ç‡æŒ‰æ¯”ä¾‹åˆ†é…ï¼ˆä¿æŒæå°å€¼ï¼‰
                normalized_pct = (original_prob / normalization_base) * 100
                
                raw_analysis[market_id] = {
                    "ai_calibrated_odds_pct": round(normalized_pct, 2),
                    "ai_confidence": 0,  # æœªåˆ†æï¼Œç½®ä¿¡åº¦ä¸º 0
                    "structural_anchor": None,
                    "noise": None,
                    "barrier": None,
                    "blindspot": None,
                    "_analyzed": False,  # æ ‡è®°ï¼šæœªè¢« AI åˆ†æï¼ˆä½äº 5% é—¨æ§›ï¼‰
                }
        
        return raw_analysis


# å•ä¾‹æ¨¡å¼
ai_analyzer = GeminiAnalyzer()
