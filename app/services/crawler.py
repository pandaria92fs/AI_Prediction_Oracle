import asyncio
import httpx
import time
import random
from datetime import datetime
from typing import List, Dict, Any

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.dialects.postgresql import insert

from app.db.session import async_session_factory
from app.models import EventSnapshot, EventCard, Tag, CardTag

# --- é…ç½®åŒºåŸŸ ---
POLYMARKET_API_URL = "https://gamma-api.polymarket.com/events"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}
# âš ï¸ å¦‚æœä½ çš„ä»£ç†ç«¯å£ä¸æ˜¯ 7890ï¼Œè¯·åœ¨è¿™é‡Œä¿®æ”¹
PROXY_URL = "http://127.0.0.1:7890" 

class PolymarketCrawler:
    def __init__(self):
        # é…ç½®ä»£ç†å’Œè¶…æ—¶
        self.client = httpx.AsyncClient(
            timeout=30.0, 
            headers=HEADERS,
            # proxies={
            #     "http://": PROXY_URL,
            #     "https://": PROXY_URL,
            # }
        )

    async def fetch_page(self, limit: int = 50, offset: int = 0):
        """æŠ“å–å•é¡µæ•°æ® (å¸¦è®¡æ—¶)"""
        params = {
            "active": "true",
            "closed": "false",
            "limit": limit,
            "offset": offset,
            "order": "volume",
            "ascending": "false",
        }

        try:
            print(f"ğŸ•·ï¸ [Offset {offset}] å‡†å¤‡å‘èµ·è¯·æ±‚...")
            
            # â±ï¸ è®¡æ—¶ç‚¹ 1: API è¯·æ±‚
            t_start = time.time()
            response = await self.client.get(POLYMARKET_API_URL, params=params)
            t_net = time.time()
            
            # æ‰“å° API è€—æ—¶
            duration = t_net - t_start
            print(f"   ğŸ“¡ [ç½‘ç»œ] Polymarket API è€—æ—¶: {duration:.2f}s")
            
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"âŒ [Offset {offset}] æŠ“å–å¤±è´¥: {str(e)}")
            return []

    async def save_batch(self, events_data: List[Dict[str, Any]]):
        """åˆ†æ‰¹å­˜å…¥æ•°æ®åº“ (ä¿®å¤æ­»é”ç‰ˆï¼šæ‰¹é‡å¤„ç† Tags)"""
        if not events_data:
            return

        t_start = time.time()

        async with async_session_factory() as session:
            try:
                # =================================================
                # ç¬¬ä¸€æ­¥ï¼šæå–æ‰€æœ‰ Tags å¹¶æ‰¹é‡å¤„ç† (è§£å†³æ­»é”æ ¸å¿ƒ)
                # =================================================
                # 1. æ”¶é›†æœ¬æ‰¹æ¬¡æ‰€æœ‰ç”¨åˆ°çš„ tag slug
                all_tag_slugs = set()
                for event in events_data:
                    for t in event.get("tags", []):
                        if t.get("slug"):
                            all_tag_slugs.add(t.get("slug"))
                
                # 2. æ’åº (å…³é”®ï¼é˜²æ­¢æ­»é”)
                sorted_slugs = sorted(list(all_tag_slugs))

                tag_map = {}  # å­˜æ”¾ name -> id çš„æ˜ å°„

                if sorted_slugs:
                    # 3. æ‰¹é‡æ’å…¥ Tags (ON CONFLICT DO NOTHING)
                    # æˆ‘ä»¬ä¸éœ€è¦åœ¨è¿™é‡Œ RETURNING idï¼Œå› ä¸ºå¯èƒ½æœ‰çš„å·²ç»å­˜åœ¨äº†ï¼ŒRETURNING ä¼šæ‹¿ä¸åˆ°
                    await session.execute(
                        insert(Tag)
                        .values([{"name": slug} for slug in sorted_slugs])
                        .on_conflict_do_nothing(index_elements=["name"])
                    )

                    # 4. æ‰¹é‡æŸ¥å‡ºæ‰€æœ‰ Tags çš„ ID
                    tag_stmt = select(Tag.name, Tag.id).where(Tag.name.in_(sorted_slugs))
                    tag_results = await session.execute(tag_stmt)
                    for name, tag_id in tag_results.all():
                        tag_map[name] = tag_id

                # =================================================
                # ç¬¬äºŒæ­¥ï¼šå¤„ç† EventCard å’Œ å…³è”å…³ç³»
                # =================================================
                for event in events_data:
                    poly_id = str(event.get("id"))
                    
                    # å­—æ®µæ¸…æ´—
                    image_url = event.get("image") or event.get("icon")
                    try:
                        volume = float(event.get("volume") or 0)
                    except:
                        volume = 0.0
                    
                    end_date = None
                    if event.get("endDate"):
                        try:
                            end_date = datetime.fromisoformat(event.get("endDate").replace("Z", "+00:00"))
                        except:
                            pass

                    # æ·»åŠ å¿«ç…§
                    session.add(EventSnapshot(polymarket_id=poly_id, raw_data=event))

                    # Upsert EventCard
                    stmt = (
                        insert(EventCard)
                        .values(
                            polymarket_id=poly_id,
                            title=event.get("title", "No Title"),
                            slug=event.get("slug", poly_id),
                            description=event.get("description"),
                            image_url=image_url,
                            volume=volume,
                            end_date=end_date,
                            is_active=event.get("active", True),
                            updated_at=datetime.utcnow(),
                        )
                        .on_conflict_do_update(
                            index_elements=["polymarket_id"],
                            set_={
                                "title": event.get("title"),
                                "volume": volume,
                                "updated_at": datetime.utcnow(),
                                "image_url": image_url,
                                "is_active": event.get("active", True)
                            },
                        )
                    )
                    
                    # è·å– Card ID
                    result = await session.execute(stmt.returning(EventCard.id))
                    card_id = result.scalar_one()

                    # å»ºç«‹å…³è” (ä½¿ç”¨å†…å­˜é‡Œçš„ tag_mapï¼Œä¸å†æŸ¥åº“)
                    raw_tags = event.get("tags", [])
                    for tag_data in raw_tags:
                        t_slug = tag_data.get("slug")
                        if t_slug and t_slug in tag_map:
                            t_id = tag_map[t_slug]
                            
                            # æ’å…¥å…³è” (å¿½ç•¥å†²çª)
                            # ä½¿ç”¨ insert è€Œä¸æ˜¯ add å¯¹è±¡ï¼Œç¨å¾®å¿«ä¸€ç‚¹
                            link_stmt = (
                                insert(CardTag)
                                .values(card_id=card_id, tag_id=t_id)
                                .on_conflict_do_nothing()
                            )
                            await session.execute(link_stmt)

                # æäº¤äº‹åŠ¡
                await session.commit()
                t_commit = time.time()
                
                # ç®—ä¸€ä¸‹è¿™ä¸€æ‰¹çš„å¹³å‡è€—æ—¶
                total_time = t_commit - t_start
                print(f"   ğŸ’¾ [æ•°æ®åº“] å†™å…¥ {len(events_data)} æ¡ | è€—æ—¶: {total_time:.2f}s")

            except Exception as e:
                await session.rollback()
                # æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯å †æ ˆï¼Œæ–¹ä¾¿è°ƒè¯•
                print(f"âŒ å…¥åº“æ‰¹æ¬¡å¤±è´¥: {str(e)}")

    async def close(self):
        await self.client.aclose()


# -------------------------------------------------
# ğŸš€ æé€Ÿå¹¶å‘æ‰§è¡Œå…¥å£
# -------------------------------------------------
async def process_batch_task(crawler, offset, semaphore):
    """å•ä¸ªæ‰¹æ¬¡ä»»åŠ¡"""
    async with semaphore:
        data = await crawler.fetch_page(limit=50, offset=offset)
        
        # ğŸ‘‡ åŠ è¿™è¡Œæ—¥å¿—
        print(f"ğŸ“„ Offset {offset}: æŠ“åˆ° {len(data)} æ¡æ•°æ®")
        
        if not data:
            return 0
        await crawler.save_batch(data)
        return len(data)

async def run_batch_crawl():
    crawler = PolymarketCrawler()
    
    # --- å‚æ•°é…ç½® ---
    TOTAL_TARGET = 1000   # ç›®æ ‡æŠ“å–æ•°é‡
    BATCH_SIZE = 50       # æ¯é¡µæ•°é‡
    CONCURRENCY = 5       # ğŸ”¥ å¹¶å‘æ•°ï¼šåŒæ—¶å‘ 5 ä¸ªè¯·æ±‚
    
    print(f"ğŸš€ å¯åŠ¨æé€Ÿçˆ¬è™« | ç›®æ ‡: {TOTAL_TARGET} | å¹¶å‘: {CONCURRENCY}")
    
    semaphore = asyncio.Semaphore(CONCURRENCY)
    offsets = range(0, TOTAL_TARGET, BATCH_SIZE)
    
    tasks = []
    for offset in offsets:
        task = process_batch_task(crawler, offset, semaphore)
        tasks.append(task)
    
    try:
        t_start = time.time()
        results = await asyncio.gather(*tasks)
        t_end = time.time()
        
        total = sum(results)
        print("-" * 40)
        print(f"ğŸ‰ ä»»åŠ¡ç»“æŸï¼å…±å¤„ç† {total} æ¡æ•°æ®")
        print(f"â±ï¸ æ€»è€—æ—¶: {t_end - t_start:.2f}s")
        print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {total / (t_end - t_start):.2f} æ¡/ç§’")
            
    finally:
        await crawler.close()

if __name__ == "__main__":
    asyncio.run(run_batch_crawl())