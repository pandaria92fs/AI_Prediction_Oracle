import asyncio
import httpx
import time
import random
from datetime import datetime
from typing import List, Dict, Any

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.dialects.postgresql import insert

from app.db.session import async_session_factory
from app.models import EventSnapshot, EventCard, Tag, CardTag

# --- é…ç½®åŒºåŸŸ ---
POLYMARKET_API_URL = "https://gamma-api.polymarket.com/events"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}
# âš ï¸ å¦‚æœä½ çš„ä»£ç†ç«¯å£ä¸æ˜¯ 7890ï¼Œè¯·åœ¨è¿™é‡Œä¿®æ”¹
PROXY_URL = "http://127.0.0.1:7890" 

class PolymarketCrawler:
    def __init__(self):
        # é…ç½®ä»£ç†å’Œè¶…æ—¶
        self.client = httpx.AsyncClient(
            timeout=30.0, 
            headers=HEADERS,
            # proxies={
            #     "http://": PROXY_URL,
            #     "https://": PROXY_URL,
            # }
        )

    async def fetch_page(self, limit: int = 50, offset: int = 0):
        """æŠ“å–å•é¡µæ•°æ® (å¸¦è®¡æ—¶)"""
        params = {
            "active": "true",
            "closed": "false",
            "limit": limit,
            "offset": offset,
            "order": "volume",
            "ascending": "false",
        }

        try:
            print(f"ğŸ•·ï¸ [Offset {offset}] å‡†å¤‡å‘èµ·è¯·æ±‚...")
            
            # â±ï¸ è®¡æ—¶ç‚¹ 1: API è¯·æ±‚
            t_start = time.time()
            response = await self.client.get(POLYMARKET_API_URL, params=params)
            t_net = time.time()
            
            # æ‰“å° API è€—æ—¶
            duration = t_net - t_start
            print(f"   ğŸ“¡ [ç½‘ç»œ] Polymarket API è€—æ—¶: {duration:.2f}s")
            
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"âŒ [Offset {offset}] æŠ“å–å¤±è´¥: {str(e)}")
            return []

    async def save_batch(self, events_data: List[Dict[str, Any]]):
        """åˆ†æ‰¹å­˜å…¥æ•°æ®åº“ (ä¿®å¤æ­»é”ç‰ˆï¼šæ‰¹é‡å¤„ç† Tags)"""
        if not events_data:
            return

        t_start = time.time()

        async with async_session_factory() as session:
            try:
                # =================================================
                # ç¬¬ä¸€æ­¥ï¼šæå–æ‰€æœ‰ Tags å¹¶æ‰¹é‡å¤„ç† (è§£å†³æ­»é”æ ¸å¿ƒ)
                # =================================================
                # 1. æ”¶é›†æœ¬æ‰¹æ¬¡æ‰€æœ‰ç”¨åˆ°çš„æ ‡ç­¾ (polymarket_id, slug)
                #    - polymarket_id: ä¸Šæ¸¸ Polymarket çš„æ ‡ç­¾ IDï¼ˆå­—ç¬¦ä¸²ï¼‰
                #    - slug: ä½œä¸ºæœ¬åœ° Tag.name å­˜å‚¨
                all_tags: dict[str, str] = {}  # polymarket_id -> slug
                for event in events_data:
                    for t in event.get("tags", []):
                        poly_tag_id = t.get("id")
                        slug = t.get("slug")
                        if poly_tag_id is None or not slug:
                            continue
                        poly_tag_id_str = str(poly_tag_id)
                        all_tags[poly_tag_id_str] = slug
                
                # 2. æŒ‰ polymarket_id æ’åº (å…³é”®ï¼é˜²æ­¢æ­»é”)
                sorted_poly_ids = sorted(all_tags.keys())

                tag_map: dict[str, int] = {}  # å­˜æ”¾ polymarket_id -> æœ¬åœ° tag.id çš„æ˜ å°„

                if sorted_poly_ids:
                    # 3. æ‰¹é‡æ’å…¥ Tags (ON CONFLICT DO UPDATE)
                    #    - polymarket_id: å”¯ä¸€çº¦æŸï¼Œç”¨äºå»é‡å’Œå…³è”
                    #    - name: å­˜ slugï¼Œä¾¿äºè°ƒè¯•/å±•ç¤º
                    #    - å¦‚æœ polymarket_id å·²å­˜åœ¨ï¼Œæ›´æ–° nameï¼ˆä»¥é˜² slug å˜åŒ–ï¼‰
                    tag_insert_stmt = insert(Tag).values(
                        [
                            {
                                "polymarket_id": poly_id,
                                "name": all_tags[poly_id],
                            }
                            for poly_id in sorted_poly_ids
                        ]
                    )
                    await session.execute(
                        tag_insert_stmt.on_conflict_do_update(
                            index_elements=["polymarket_id"],
                            set_={"name": tag_insert_stmt.excluded.name},
                        )
                    )

                    # 4. æ‰¹é‡æŸ¥å‡ºæ‰€æœ‰ Tags çš„ IDï¼ˆé€šè¿‡ polymarket_idï¼‰
                    tag_stmt = select(Tag.polymarket_id, Tag.id).where(
                        Tag.polymarket_id.in_(sorted_poly_ids)
                    )
                    tag_results = await session.execute(tag_stmt)
                    for poly_id, tag_id in tag_results.all():
                        tag_map[poly_id] = tag_id

                # =================================================
                # ç¬¬äºŒæ­¥ï¼šå¤„ç† EventCard å’Œ EventSnapshot
                # =================================================
                # æ”¶é›† card_id æ˜ å°„ï¼Œç”¨äºåç»­æ‰¹é‡æ’å…¥å…³è”
                event_card_ids: dict[str, int] = {}  # polymarket_id -> card_id
                
                for event in events_data:
                    poly_id = str(event.get("id"))
                    
                    # å­—æ®µæ¸…æ´—
                    image_url = event.get("image") or event.get("icon")
                    try:
                        volume = float(event.get("volume") or 0)
                    except:
                        volume = 0.0
                    
                    end_date = None
                    if event.get("endDate"):
                        try:
                            end_date = datetime.fromisoformat(event.get("endDate").replace("Z", "+00:00"))
                        except:
                            pass

                    # æ·»åŠ å¿«ç…§
                    session.add(EventSnapshot(polymarket_id=poly_id, raw_data=event))

                    # Upsert EventCard
                    stmt = (
                        insert(EventCard)
                        .values(
                            polymarket_id=poly_id,
                            title=event.get("title", "No Title"),
                            slug=event.get("slug", poly_id),
                            description=event.get("description"),
                            image_url=image_url,
                            volume=volume,
                            end_date=end_date,
                            is_active=event.get("active", True),
                            updated_at=datetime.utcnow(),
                        )
                        .on_conflict_do_update(
                            index_elements=["polymarket_id"],
                            set_={
                                "title": event.get("title"),
                                "volume": volume,
                                "updated_at": datetime.utcnow(),
                                "image_url": image_url,
                                "is_active": event.get("active", True)
                            },
                        )
                    )
                    
                    # è·å– Card ID
                    result = await session.execute(stmt.returning(EventCard.id))
                    card_id = result.scalar_one()
                    event_card_ids[poly_id] = card_id

                # =================================================
                # ç¬¬ä¸‰æ­¥ï¼šæ‰¹é‡æ’å…¥å…³è”å…³ç³» (ä½¿ç”¨ tag_map)
                # =================================================
                card_tag_links = []
                for event in events_data:
                    poly_id = str(event.get("id"))
                    card_id = event_card_ids.get(poly_id)
                    if not card_id:
                        continue
                    
                    for tag_data in event.get("tags", []):
                        poly_tag_id = tag_data.get("id")
                        if poly_tag_id is None:
                            continue
                        poly_tag_id_str = str(poly_tag_id)
                        if poly_tag_id_str in tag_map:
                            card_tag_links.append({
                                "card_id": card_id,
                                "tag_id": tag_map[poly_tag_id_str],
                            })
                
                # æ‰¹é‡æ’å…¥å…³è” (å¿½ç•¥å†²çª)
                if card_tag_links:
                    await session.execute(
                        insert(CardTag).values(card_tag_links).on_conflict_do_nothing()
                    )

                # æäº¤äº‹åŠ¡
                await session.commit()
                t_commit = time.time()
                
                # ç®—ä¸€ä¸‹è¿™ä¸€æ‰¹çš„å¹³å‡è€—æ—¶
                total_time = t_commit - t_start
                print(f"   ğŸ’¾ [æ•°æ®åº“] å†™å…¥ {len(events_data)} æ¡ | è€—æ—¶: {total_time:.2f}s")

            except Exception as e:
                await session.rollback()
                # æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯å †æ ˆï¼Œæ–¹ä¾¿è°ƒè¯•
                print(f"âŒ å…¥åº“æ‰¹æ¬¡å¤±è´¥: {str(e)}")

    async def close(self):
        await self.client.aclose()


# -------------------------------------------------
# ğŸš€ æé€Ÿå¹¶å‘æ‰§è¡Œå…¥å£
# -------------------------------------------------
async def process_batch_task(crawler, offset, semaphore):
    """å•ä¸ªæ‰¹æ¬¡ä»»åŠ¡"""
    async with semaphore:
        data = await crawler.fetch_page(limit=50, offset=offset)
        
        # ğŸ‘‡ åŠ è¿™è¡Œæ—¥å¿—
        print(f"ğŸ“„ Offset {offset}: æŠ“åˆ° {len(data)} æ¡æ•°æ®")
        
        if not data:
            return 0
        await crawler.save_batch(data)
        return len(data)

async def run_batch_crawl():
    crawler = PolymarketCrawler()
    
    # --- å‚æ•°é…ç½® ---
    TOTAL_TARGET = 1000   # ç›®æ ‡æŠ“å–æ•°é‡
    BATCH_SIZE = 50       # æ¯é¡µæ•°é‡
    CONCURRENCY = 5       # ğŸ”¥ å¹¶å‘æ•°ï¼šåŒæ—¶å‘ 5 ä¸ªè¯·æ±‚
    
    print(f"ğŸš€ å¯åŠ¨æé€Ÿçˆ¬è™« | ç›®æ ‡: {TOTAL_TARGET} | å¹¶å‘: {CONCURRENCY}")
    
    semaphore = asyncio.Semaphore(CONCURRENCY)
    offsets = range(0, TOTAL_TARGET, BATCH_SIZE)
    
    tasks = []
    for offset in offsets:
        task = process_batch_task(crawler, offset, semaphore)
        tasks.append(task)
    
    try:
        t_start = time.time()
        results = await asyncio.gather(*tasks)
        t_end = time.time()
        
        total = sum(results)
        print("-" * 40)
        print(f"ğŸ‰ ä»»åŠ¡ç»“æŸï¼å…±å¤„ç† {total} æ¡æ•°æ®")
        print(f"â±ï¸ æ€»è€—æ—¶: {t_end - t_start:.2f}s")
        print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {total / (t_end - t_start):.2f} æ¡/ç§’")
            
    finally:
        await crawler.close()

if __name__ == "__main__":
    asyncio.run(run_batch_crawl())