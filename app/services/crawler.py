import asyncio
import httpx
import time
import json
from datetime import datetime
from typing import List, Dict, Any, Optional

from sqlalchemy import select, delete
from sqlalchemy.dialects.postgresql import insert

from app.db.session import async_session_factory
from app.models import EventSnapshot, EventCard, Tag, CardTag, AIPrediction
from app.services.gemini_analyzer import ai_analyzer

# --- é…ç½®åŒºåŸŸ ---
POLYMARKET_API_URL = "https://gamma-api.polymarket.com/events"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

class PolymarketCrawler:
    def __init__(self):
        self.client = httpx.AsyncClient(
            timeout=30.0, 
            headers=HEADERS,
        )

    # ==========================================
    # ğŸ‘‡ æ–°å¢ï¼šå¸‚åœºæ•°æ®æ¸…æ´—é€»è¾‘
    # ==========================================
    def _get_market_odds(self, market: Dict[str, Any]) -> float:
        """ä»å¸‚åœºæ•°æ®ä¸­æå–å½“å‰èµ”ç‡ (ä¼˜å…ˆçº§: lastTradePrice > bestBid > outcomePrices)"""
        # 1. å°è¯• lastTradePrice
        if 'lastTradePrice' in market and market['lastTradePrice'] is not None:
            try:
                return float(market['lastTradePrice'])
            except (ValueError, TypeError):
                pass
        
        # 2. å°è¯• bestBid
        if 'bestBid' in market and market['bestBid']:
            try:
                return float(market['bestBid'])
            except (ValueError, TypeError):
                pass
        
        # 3. å°è¯• outcomePrices
        if 'outcomePrices' in market:
            outcome_prices = market['outcomePrices']
            if isinstance(outcome_prices, str):
                try:
                    outcome_prices = json.loads(outcome_prices)
                except:
                    pass
            if isinstance(outcome_prices, list) and len(outcome_prices) > 0:
                try:
                    return float(outcome_prices[0])
                except:
                    pass
        return 0.0

    def _preprocess_event_for_ai(self, event: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        æ ¸å¿ƒç­›é€‰é€»è¾‘ï¼š
        1. è¿‡æ»¤æ— æ•ˆå¸‚åœº (archived, closed)
        2. æŒ‰èµ”ç‡ä»é«˜åˆ°ä½æ’åº
        3. ä»…ä¿ç•™èµ”ç‡ >= 0.05 çš„å¸‚åœº (è‹¥ä¸è¶³2ä¸ªåˆ™å–å‰2ï¼Œè‹¥è¶…è¿‡5ä¸ªåˆ™å–å‰5)
        """
        markets = event.get("markets", [])
        if not markets:
            return None
        
        # 1. åŸºç¡€è¿‡æ»¤ï¼šå¿…é¡»æ˜¯æ´»è·ƒä¸”æœªå…³é—­çš„
        eligible_markets = []
        for market in markets:
            if market.get('archived') is True: continue
            if market.get('active') is not True: continue
            if market.get('closed') is True: continue
            eligible_markets.append(market)

        if not eligible_markets:
            return None

        # 2. è®¡ç®—èµ”ç‡å¹¶é™„åŠ å…ƒæ•°æ®
        all_markets_with_odds = []
        for market in eligible_markets:
            odds = self._get_market_odds(market)
            # ä¿ç•™åŸå§‹ market å¯¹è±¡é‡Œçš„æ‰€æœ‰å­—æ®µï¼Œå¹¶æ›´æ–° calculated_odds
            market_copy = market.copy()
            market_copy['calculated_odds'] = odds
            # åŒæ—¶ä¹ŸæŠŠè¿™ä¸ªèµ”ç‡å¡å› outcomePrices æ ¼å¼ï¼Œé€‚é… gemini_analyzer çš„è¯»å–é€»è¾‘
            market_copy['outcomePrices'] = [str(odds), str(1-odds)] 
            
            all_markets_with_odds.append(market_copy)

        # 3. æŒ‰èµ”ç‡é™åºæ’åº
        all_markets_with_odds.sort(key=lambda x: x['calculated_odds'], reverse=True)

        # 4. æ™ºèƒ½æˆªå–é€»è¾‘
        # è§„åˆ™ A: å…ˆæ‰¾æ‰€æœ‰èµ”ç‡ >= 5% çš„
        filtered_markets = [m for m in all_markets_with_odds if m['calculated_odds'] >= 0.05]

        # è§„åˆ™ B: æ•°é‡æ§åˆ¶
        if len(filtered_markets) < 2:
            # å¦‚æœç¬¦åˆæ¡ä»¶çš„å¤ªå°‘ï¼Œè‡³å°‘å–å‰ 2 ä¸ª (çŸ®å­é‡Œæ‹”å°†å†›)
            final_markets = all_markets_with_odds[:2]
        elif len(filtered_markets) > 5:
            # å¦‚æœç¬¦åˆæ¡ä»¶çš„å¤ªå¤šï¼Œåªå–å‰ 5 ä¸ª (å¤´éƒ¨èšç„¦)
            final_markets = filtered_markets[:5]
        else:
            final_markets = filtered_markets
            
        return {
            "title": event.get("title"),
            "description": event.get("description"),
            "markets": final_markets
        }

    # ==========================================
    # ğŸ‘† æ–°å¢ç»“æŸ
    # ==========================================

    async def fetch_page(self, limit: int = 50, offset: int = 0):
        params = {
            "active": "true",
            "closed": "false",
            "limit": limit,
            "offset": offset,
            "order": "volume",
            "ascending": "false",
        }
        try:
            print(f"ğŸ•·ï¸ [Offset {offset}] å‡†å¤‡å‘èµ·è¯·æ±‚...")
            t_start = time.time()
            response = await self.client.get(POLYMARKET_API_URL, params=params)
            t_net = time.time()
            print(f"   ğŸ“¡ [ç½‘ç»œ] Polymarket API è€—æ—¶: {t_net - t_start:.2f}s")
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"âŒ [Offset {offset}] æŠ“å–å¤±è´¥: {str(e)}")
            return []

    async def save_batch(self, events_data: List[Dict[str, Any]]):
        if not events_data: return

        t_start = time.time()
        event_card_ids: dict[str, int] = {}

        async with async_session_factory() as session:
            try:
                # --- 1. Tags å¤„ç† ---
                all_tags: dict[str, str] = {}
                for event in events_data:
                    for t in event.get("tags", []):
                        if t.get("id") and t.get("slug"):
                            all_tags[str(t.get("id"))] = t.get("slug")
                
                sorted_poly_ids = sorted(all_tags.keys())
                tag_map: dict[str, int] = {}

                if sorted_poly_ids:
                    tag_insert_stmt = insert(Tag).values([
                        {"polymarket_id": pid, "name": all_tags[pid]} 
                        for pid in sorted_poly_ids
                    ])
                    await session.execute(
                        tag_insert_stmt.on_conflict_do_update(
                            index_elements=["polymarket_id"],
                            set_={"name": tag_insert_stmt.excluded.name},
                        )
                    )
                    tag_stmt = select(Tag.polymarket_id, Tag.id).where(Tag.polymarket_id.in_(sorted_poly_ids))
                    for pid, tid in (await session.execute(tag_stmt)).all():
                        tag_map[pid] = tid

                # --- 2. EventCard å¤„ç† ---
                for event in events_data:
                    poly_id = str(event.get("id"))
                    image_url = event.get("image") or event.get("icon")
                    volume = float(event.get("volume") or 0)
                    end_date = None
                    if event.get("endDate"):
                        try:
                            end_date = datetime.fromisoformat(event.get("endDate").replace("Z", "+00:00"))
                        except: pass

                    session.add(EventSnapshot(polymarket_id=poly_id, raw_data=event))

                    stmt = (
                        insert(EventCard).values(
                            polymarket_id=poly_id,
                            title=event.get("title", "No Title"),
                            slug=event.get("slug", poly_id),
                            description=event.get("description"),
                            image_url=image_url,
                            volume=volume,
                            end_date=end_date,
                            is_active=event.get("active", True),
                            updated_at=datetime.utcnow(),
                        ).on_conflict_do_update(
                            index_elements=["polymarket_id"],
                            set_={
                                "title": event.get("title"),
                                "volume": volume,
                                "updated_at": datetime.utcnow(),
                                "image_url": image_url,
                                "is_active": event.get("active", True)
                            },
                        )
                    )
                    card_id = (await session.execute(stmt.returning(EventCard.id))).scalar_one()
                    event_card_ids[poly_id] = card_id

                # --- 3. å…³è” Tags ---
                card_tag_links = []
                for event in events_data:
                    cid = event_card_ids.get(str(event.get("id")))
                    if not cid: continue
                    for t in event.get("tags", []):
                        tid = tag_map.get(str(t.get("id")))
                        if tid: card_tag_links.append({"card_id": cid, "tag_id": tid})
                
                if card_tag_links:
                    await session.execute(insert(CardTag).values(card_tag_links).on_conflict_do_nothing())

                await session.commit()
                print(f"   ğŸ’¾ [æ•°æ®åº“] å†™å…¥ {len(events_data)} æ¡ | è€—æ—¶: {time.time() - t_start:.2f}s")

            except Exception as e:
                await session.rollback()
                print(f"âŒ å…¥åº“å¤±è´¥: {str(e)}")
                return

        # --- 4. è§¦å‘ AI åˆ†æ ---
        if event_card_ids:
            await self._process_ai_analysis(events_data, event_card_ids)

    async def _process_ai_analysis(self, events_data: List[Dict[str, Any]], event_card_ids: Dict[str, int]):
        """å¤„ç† AI åˆ†æ (åº”ç”¨äº†é¢„å¤„ç†ç­›é€‰)"""
        if not ai_analyzer.api_key: return

        async with async_session_factory() as session:
            try:
                for event in events_data:
                    poly_id = str(event.get("id"))
                    card_id = event_card_ids.get(poly_id)
                    if not card_id: continue

                    # -------------------------------------------------------
                    # ğŸ‘‡ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨é¢„å¤„ç†å‡½æ•°ç­›é€‰ Markets
                    # -------------------------------------------------------
                    filtered_event_data = self._preprocess_event_for_ai(event)
                    
                    # å¦‚æœç­›é€‰åæ²¡æœ‰æœ‰æ•ˆå¸‚åœº (æ¯”å¦‚éƒ½å…³é—­äº†)ï¼Œåˆ™è·³è¿‡ AI åˆ†æ
                    if not filtered_event_data or not filtered_event_data['markets']:
                        continue

                    try:
                        await asyncio.sleep(0.5) # é™æµ
                        # ä¼ å…¥çš„æ˜¯ç­›é€‰åçš„æ•°æ®ï¼ŒAI åªä¼šåˆ†æè¿™å‡ ä¸ª
                        ai_result = await ai_analyzer.analyze_event(filtered_event_data)
                    except Exception as e:
                        print(f"   âš ï¸ AI è¯·æ±‚å¤±è´¥: {e}")
                        continue

                    if not ai_result: continue

                    # --- åç»­å…¥åº“é€»è¾‘ ---
                    summary = ai_result.get("executive_summary", "")
                    markets_data = ai_result.get("markets", {})
                    
                    primary_prediction = "0"
                    primary_conf = 0.0
                    for _, mdata in markets_data.items():
                        conf = mdata.get("confidence_score", 0)
                        if conf > primary_conf:
                            primary_conf = conf
                            odds = mdata.get("ai_calibrated_odds", 0) * 100
                            primary_prediction = f"{odds:.1f}"

                    raw_analysis = ai_analyzer.transform_to_raw_analysis(ai_result)
                    
                    # å›å¡«åŸå§‹æ•°æ®
                    all_original_markets = event.get("markets", [])
                    for market in all_original_markets:
                        m_id = str(market.get("id", ""))
                        if m_id in raw_analysis:
                            raw_analysis[m_id]["question"] = market.get("question", "")
                            odds = self._get_market_odds(market)
                            raw_analysis[m_id]["original_odds"] = odds

                    await session.execute(delete(AIPrediction).where(AIPrediction.card_id == card_id))
                    session.add(AIPrediction(
                        card_id=card_id,
                        summary=summary,
                        outcome_prediction=primary_prediction,
                        confidence_score=min(primary_conf * 10, 99.9),
                        raw_analysis=json.dumps(raw_analysis, ensure_ascii=False)
                    ))
                    print(f"   ğŸ¤– AI åˆ†æå®Œæˆ: {event.get('title', '')[:30]}... (åŸºäº Top {len(filtered_event_data['markets'])} å¸‚åœº)")

                await session.commit()
            except Exception as e:
                await session.rollback()
                print(f"âŒ AI åˆ†ææ‰¹æ¬¡å¤±è´¥: {e}")

    async def close(self):
        await self.client.aclose()

# -------------------------------------------------
# ğŸš€ æé€Ÿå¹¶å‘æ‰§è¡Œå…¥å£
# -------------------------------------------------
async def process_batch_task(crawler, offset, semaphore):
    async with semaphore:
        data = await crawler.fetch_page(limit=50, offset=offset)
        print(f"ğŸ“„ Offset {offset}: æŠ“åˆ° {len(data)} æ¡æ•°æ®")
        if not data: return 0
        await crawler.save_batch(data)
        return len(data)

async def run_batch_crawl():
    crawler = PolymarketCrawler()
    # ç”Ÿäº§ç¯å¢ƒé…ç½®
    TOTAL_TARGET = 200   
    BATCH_SIZE = 50       
    CONCURRENCY = 5       
    
    print(f"ğŸš€ å¯åŠ¨æé€Ÿçˆ¬è™« (å¸¦ AI æ™ºèƒ½ç­›é€‰) | ç›®æ ‡: {TOTAL_TARGET} | å¹¶å‘: {CONCURRENCY}")
    
    semaphore = asyncio.Semaphore(CONCURRENCY)
    offsets = range(0, TOTAL_TARGET, BATCH_SIZE)
    tasks = [process_batch_task(crawler, offset, semaphore) for offset in offsets]
    
    try:
        t_start = time.time()
        results = await asyncio.gather(*tasks)
        total = sum(results)
        print("-" * 40)
        print(f"ğŸ‰ ä»»åŠ¡ç»“æŸï¼å…±å¤„ç† {total} æ¡æ•°æ® | è€—æ—¶: {time.time() - t_start:.2f}s")
    finally:
        await crawler.close()

if __name__ == "__main__":
    asyncio.run(run_batch_crawl())
